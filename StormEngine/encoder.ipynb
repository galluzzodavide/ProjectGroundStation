{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c3e8ab",
   "metadata": {},
   "source": [
    "This encoder produces two outputs: \n",
    "cls_out (a global feature vector summarizing the state) and\n",
    " feature_map (a smaller spatial feature map of size \n",
    "H/patch Ã— W/patch capturing spatial details).\n",
    " We will use \n",
    "feature_map as input to the next stage, and use \n",
    "cls_out to initialize hidden\n",
    " states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bca5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ViTEncoder(nn.Module):\n",
    "def __init__(self, img_size, patch_size, in_channels, embed_dim, num_layers=6, num_heads=8):\n",
    "    super().__init__()\n",
    "    assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "    self.patch_size = patch_size\n",
    "    self.num_patches = (img_size // patch_size) ** 2\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    # Patch embedding: conv layer that produces embed_dim feature maps from input channels\n",
    "    self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    # Class token and positional embedding\n",
    "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "    self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    " \n",
    "    # Transformer Encoder\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=4*embed_dim)\n",
    "    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    \n",
    "def forward(self, x):\n",
    "    # x shape: (B, in_channels, H, W)\n",
    "    B = x.size(0)\n",
    "    # Create patch embeddings\n",
    "    patches = self.patch_embed(x)\n",
    "    # (B, embed_dim, H/patch, W/ patch)\n",
    "\n",
    "    patches = patches.flatten(2).transpose(1, 2) # (B, N, embed_dim), \n",
    "    N=num_patches\n",
    "    \n",
    "    # Prepend class token\n",
    "    cls_tokens = self.cls_token.expand(B,-1,-1) # (B, 1, embed_dim)\n",
    "    tokens = torch.cat([cls_tokens, patches], dim=1) # (B, N+1, embed_dim)\n",
    "    tokens = tokens + self.pos_embed[:, :tokens.size(1), :]\n",
    " \n",
    "    # Transformer encoding\n",
    "    tokens = tokens.transpose(0, 1)\n",
    "    # (N+1, B, embed_dim) for transformer\n",
    "    enc_outputs = self.transformer(tokens)\n",
    "    enc_outputs = enc_outputs.transpose(0, 1)\n",
    "    # Separate class token and patch embeddings\n",
    "    cls_out = enc_outputs[:, 0, :]\n",
    "    # (N+1, B, embed_dim)\n",
    "    # (B, N+1, embed_dim)\n",
    "    # (B, embed_dim)\n",
    " patch_out = enc_outputs[:, 1:, :].transpose(1, 2) # (B, embed_dim, N)\n",
    " \n",
    " # Reshape patch_out back to spatial grid\n",
    " grid_size = int(self.num_patches**0.5)\n",
    " feature_map = patch_out.view(B, self.embed_dim, grid_size, grid_size)\n",
    " \n",
    " # (B, embed_dim, H/patch, W/patch)\n",
    " return cls_out, feature_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
