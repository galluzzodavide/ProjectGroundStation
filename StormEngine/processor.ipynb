{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5db9f32",
   "metadata": {},
   "source": [
    "The Processor is the core that learns temporal dynamics and produces a coarse forecast for up to 72 hours.\n",
    "\n",
    "In our design, for simplicity, we assume the model is autoregressive: it takes the current state (at time t0) and then iteratively predicts the next 72 hourly states. The encoder (ViT) has already given us an encoding of the current state\n",
    "\n",
    " We feed this encoding into the ConvLSTM as the initial input. The\n",
    " ConvLSTM will output a prediction for the next time step, which we then loop back as input for the\n",
    " following step, and so on, generating forecasts up to +72h. This approach mimics how NWP uses the\n",
    " current analysis to step the model forward in time.\n",
    "\n",
    " ------------------------------------------------------------------------------------------------------------------------------------\n",
    "ConvLSTM Implementation: PyTorch does not have a built-in ConvLSTM in \n",
    "nn module, so we\n",
    " implement it. A single ConvLSTM cell can be coded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "def __init__(self, input_channels, hidden_channels, kernel_size=3):\n",
    "    super().__init__()\n",
    "    self.hidden_channels = hidden_channels\n",
    "    # Combine input and hidden for gating\n",
    "    self.conv = nn.Conv2d(input_channels + hidden_channels, 4 *hidden_channels,\n",
    "    kernel_size, padding=kernel_size//2)\n",
    "def forward(self, x, h_prev, c_prev):\n",
    "    # x: input tensor (B, input_ch, H, W)\n",
    "    # h_prev, c_prev: previous hidden and cell state (B, hidden_ch, H, W)\n",
    "    if h_prev is None:\n",
    "    # initialize with zeros if not provided\n",
    "        B, _, H, W = x.shape\n",
    "        h_prev = x.new_zeros(B, self.hidden_channels, H, W)\n",
    "        c_prev = x.new_zeros(B, self.hidden_channels, H, W)\n",
    "    combined = torch.cat([x, h_prev], dim=1) # concatenate along channel\n",
    "    gates = self.conv(combined) # shape (B, 4*hidden_ch, H, W)\n",
    "    # Split gates\n",
    "    Ci = self.hidden_channels\n",
    "    input_gate = torch.sigmoid(gates[:, :Ci])\n",
    "    forget_gate = torch.sigmoid(gates[:, Ci:2*Ci])\n",
    "    output_gate = torch.sigmoid(gates[:, 2*Ci:3*Ci])\n",
    "    candidate = torch.tanh(gates[:, 3*Ci:4*Ci])\n",
    "    c_new = forget_gate * c_prev + input_gate * candidate\n",
    "    h_new = output_gate * torch.tanh(c_new)\n",
    "    return h_new, c_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11fe817",
   "metadata": {},
   "source": [
    " Using this cell, we can build a multi-layer ConvLSTM (stacking cells, where each layerâ€™s hidden state\n",
    " feeds into the next). For forecasting, a Decoder loop runs the ConvLSTM step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMForecast(nn.Module):\n",
    "def __init__(self, input_ch, hidden_ch):\n",
    "    super().__init__()\n",
    "    self.cell = ConvLSTMCell(input_ch, hidden_ch)\n",
    " def forward(self, init_input, steps=48):\n",
    "    # init_input: (B, input_ch, H, W) at t0\n",
    "    h, c = None, None # will be initialized in cell\n",
    "    x = init_input\n",
    "    outputs = []\n",
    " for t in range(steps):\n",
    "    h, c = self.cell(x, h, c) \n",
    "    outputs.append(h)        # use hidden state as output prediction\n",
    "    x = h                   # feed output as next input (autoregressive)\n",
    "    outputs = torch.stack(outputs, dim=1) # (B, steps, hidden_ch, H, W)\n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
